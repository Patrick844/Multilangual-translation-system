

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>lingowiz.train &mdash; LingoWiz  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            LingoWiz
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../lingowiz.html">lingowiz package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">lingowiz</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">LingoWiz</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">lingowiz.train</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for lingowiz.train</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Module: train.py</span>

<span class="sd">This module provides a complete pipeline for fine-tuning machine translation models</span>
<span class="sd">using Hugging Face&#39;s ``transformers`` library and managing training experiments with MLflow.</span>
<span class="sd">It supports data preprocessing, tokenization, model training, and logging results to Hugging Face</span>
<span class="sd">and MLflow.</span>

<span class="sd">Features:</span>
<span class="sd">---------</span>
<span class="sd">1. **Data Splitting**:</span>
<span class="sd">    - Splits data into training and evaluation sets.</span>

<span class="sd">2. **Model Training**:</span>
<span class="sd">    - Fine-tunes a MarianMT model on custom datasets.</span>
<span class="sd">    - Supports freezing encoder and decoder layers for efficient fine-tuning.</span>

<span class="sd">3. **Tokenization**:</span>
<span class="sd">    - Tokenizes input and target sentences for training and evaluation.</span>

<span class="sd">4. **MLflow Integration**:</span>
<span class="sd">    - Logs training hyperparameters, metrics, and models.</span>
<span class="sd">    - Automatically creates or updates MLflow experiments.</span>

<span class="sd">5. **Hugging Face Integration**:</span>
<span class="sd">    - Saves and uploads trained models to the Hugging Face Model Hub.</span>

<span class="sd">6. **Callbacks**:</span>
<span class="sd">    - Includes a callback to clear GPU memory after each epoch.</span>

<span class="sd">Dependencies:</span>
<span class="sd">-------------</span>
<span class="sd">- ``transformers``: For Hugging Face models and training utilities.</span>
<span class="sd">- ``mlflow``: For experiment tracking and logging.</span>
<span class="sd">- ``pandas``: For data manipulation.</span>
<span class="sd">- ``datasets``: For handling datasets in Hugging Face format.</span>
<span class="sd">- ``torch``: For GPU/CPU compatibility during training.</span>
<span class="sd">- ``tqdm``: For progress visualization.</span>
<span class="sd">- ``requests``: For making API calls to external services.</span>

<span class="sd">Example Usage:</span>
<span class="sd">--------------</span>
<span class="sd">This module is designed for training machine translation models on custom datasets, integrating</span>
<span class="sd">key utilities for tokenization, training, logging, and uploading models.</span>

<span class="sd">.. code-block:: python</span>

<span class="sd">    from translation_training_pipeline import training_pipeline</span>

<span class="sd">    df = pd.read_csv(&quot;translation_data.csv&quot;)</span>
<span class="sd">    training_pipeline(</span>
<span class="sd">        df=df,</span>
<span class="sd">        src=&quot;English&quot;,</span>
<span class="sd">        base_model=&quot;Helsinki-NLP/opus-mt-en-ar&quot;,</span>
<span class="sd">        steps=1000,</span>
<span class="sd">        batch_size=16,</span>
<span class="sd">        learning_rate=5e-5,</span>
<span class="sd">        epochs=3,</span>
<span class="sd">        warmup=100,</span>
<span class="sd">        trg_language=&quot;Arabic&quot;,</span>
<span class="sd">        layer=1</span>
<span class="sd">    )</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Importing Librairies</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="c1"># HF librairies for fine tuning</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MarianTokenizer</span><span class="p">,</span>
                          <span class="n">MarianMTModel</span><span class="p">,</span>
                          <span class="n">Trainer</span><span class="p">,</span>
                          <span class="n">TrainingArguments</span><span class="p">,</span>
                          <span class="n">TrainerCallback</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.data.pandas_dataset</span> <span class="kn">import</span> <span class="n">PandasDataset</span>
<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">HfApi</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">torch</span>


<span class="c1"># Suppress all UserWarnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WANDB_MODE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;disabled&quot;</span>
<span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">token</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;HF_TOKEN&#39;</span><span class="p">)</span>
<span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">()</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span><span class="n">disable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span><span class="n">disable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span><span class="n">disable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<div class="viewcode-block" id="split_test">
<a class="viewcode-back" href="../../lingowiz.html#lingowiz.train.split_test">[docs]</a>
<span class="k">def</span> <span class="nf">split_test</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the DataFrame into training and testing sets.</span>

<span class="sd">    Args:</span>
<span class="sd">        df (pd.DataFrame): The input DataFrame to split.</span>
<span class="sd">        test_size (float, optional): Fraction of the data to reserve for testing. Defaults to 0.2.</span>
<span class="sd">        random_state (int, optional): Random seed for reproducibility. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple: A tuple containing:</span>
<span class="sd">            - pd.DataFrame: Training set.</span>
<span class="sd">            - pd.DataFrame: Testing set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span>
                                         <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span>
                                         <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span></div>



<div class="viewcode-block" id="EmptyCacheCallback">
<a class="viewcode-back" href="../../lingowiz.html#lingowiz.train.EmptyCacheCallback">[docs]</a>
<span class="k">class</span> <span class="nc">EmptyCacheCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A callback to clear GPU memory after each training epoch to prevent memory overflow.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="EmptyCacheCallback.on_epoch_end">
<a class="viewcode-back" href="../../lingowiz.html#lingowiz.train.EmptyCacheCallback.on_epoch_end">[docs]</a>
    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clears the GPU cache after the end of an epoch.</span>

<span class="sd">        Args:</span>
<span class="sd">            args (TrainingArguments): Training arguments.</span>
<span class="sd">            state (TrainerState): Trainer state.</span>
<span class="sd">            control (TrainerControl): Trainer control object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Clearing cache after epoch </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span></div>
</div>



<div class="viewcode-block" id="train">
<a class="viewcode-back" href="../../lingowiz.html#lingowiz.train.train">[docs]</a>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
          <span class="n">tokenized_datasets_train</span><span class="p">,</span>
          <span class="n">tokenized_datasets_eval</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="p">,</span>
          <span class="n">lr</span><span class="p">,</span>
          <span class="n">epochs</span><span class="p">,</span>
          <span class="n">warmup</span><span class="p">,</span>
          <span class="n">tokenizer</span><span class="p">):</span>
    <span class="c1"># Adjust training arguments for small dataset</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trains a MarianMT model on tokenized datasets.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (MarianMTModel): The MarianMT model to fine-tune.</span>
<span class="sd">        tokenized_datasets_train (Dataset): Tokenized training dataset.</span>
<span class="sd">        tokenized_datasets_eval (Dataset): Tokenized evaluation dataset.</span>
<span class="sd">        steps (int): Total training steps.</span>
<span class="sd">        batch_size (int): Batch size for training and evaluation.</span>
<span class="sd">        lr (float): Learning rate for optimization.</span>
<span class="sd">        epochs (int): Number of training epochs.</span>
<span class="sd">        warmup (int): Number of warmup steps for learning rate scheduling.</span>
<span class="sd">        tokenizer (MarianTokenizer): Tokenizer for the MarianMT model.</span>
<span class="sd">        bool_model (bool): Reserved for additional configuration (unused in this function).</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple: A tuple containing:</span>
<span class="sd">            - Trainer: Hugging Face Trainer object after training.</span>
<span class="sd">            - TrainOutput: Training metrics.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializaing Training Arguments ...&quot;</span><span class="p">)</span>

    <span class="c1"># Directory to save training checkpoints and outputs</span>
    <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;temp&quot;</span>

    <span class="c1"># Evaluate the model at the end of each epoch</span>
    <span class="n">evaluation_strategy</span> <span class="o">=</span> <span class="s2">&quot;epoch&quot;</span>

    <span class="c1"># Log training metrics at the end of each epoch</span>
    <span class="n">logging_strategy</span> <span class="o">=</span> <span class="s2">&quot;epoch&quot;</span>

    <span class="c1"># Batch size for training per device (GPU/CPU)</span>
    <span class="n">per_device_train_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>

    <span class="c1"># Batch size for evaluation per device (GPU/CPU)</span>
    <span class="n">per_device_eval_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>

    <span class="c1"># Learning rate for the optimizer during fine-tuning</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="c1"># Number of epochs for training</span>
    <span class="n">num_train_epochs</span> <span class="o">=</span> <span class="n">epochs</span>

    <span class="c1"># Number of steps for learning rate warmup</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup</span>

    <span class="c1"># Use mixed precision (16-bit floating point) for faster training</span>
    <span class="n">fp16</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Initialize training arguments for the Hugging Face Trainer</span>
    <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span><span class="o">=</span><span class="n">evaluation_strategy</span><span class="p">,</span>
        <span class="n">logging_strategy</span><span class="o">=</span><span class="n">logging_strategy</span><span class="p">,</span>
        <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">per_device_train_batch_size</span><span class="p">,</span>
        <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">per_device_eval_batch_size</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">num_train_epochs</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
        <span class="n">fp16</span><span class="o">=</span><span class="n">fp16</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets_train</span><span class="p">,</span>
        <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets_eval</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">EmptyCacheCallback</span><span class="p">()]</span>
    <span class="p">)</span>

    <span class="c1"># Train the model</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training ...&quot;</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="c1"># Return the trainer</span>
    <span class="k">return</span> <span class="n">trainer</span></div>



<span class="c1"># Tokenization function for both source (English) and target (Arabic)</span>
<span class="c1"># transforming sentences into list of words</span>
<div class="viewcode-block" id="tokenize_function">
<a class="viewcode-back" href="../../lingowiz.html#lingowiz.train.tokenize_function">[docs]</a>
<span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tokenizes source and target texts for training and evaluation.</span>

<span class="sd">    Args:</span>
<span class="sd">        data (Dataset): The dataset containing source and target texts.</span>
<span class="sd">        tokenizer (MarianTokenizer): Tokenizer for the MarianMT model.</span>
<span class="sd">        src (str): Name of the source language column in the dataset.</span>
<span class="sd">        trg (str): Name of the target language column in the dataset.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: Tokenized input and target sequences, including labels.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Tokenize the source (English) text</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">src</span><span class="p">],</span>
                       <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
                       <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

    <span class="c1"># Tokenize the target (Arabic) text</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">trg</span><span class="p">],</span>
                        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
                        <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

    <span class="c1"># Set the &#39;labels&#39; field to the tokenized target (Arabic) text</span>
    <span class="n">source</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">source</span></div>



<div class="viewcode-block" id="split_eval">
<a class="viewcode-back" href="../../lingowiz.html#lingowiz.train.split_eval">[docs]</a>
<span class="k">def</span> <span class="nf">split_eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">eval_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a dataset into training and evaluation subsets.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset (Dataset): The dataset to split.</span>
<span class="sd">        eval_size (float, optional): Proportion of the dataset for evaluation. Defaults to 0.2.</span>
<span class="sd">        random_state (int, optional): Random seed for reproducibility. Defaults to 42.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple: A tuple containing:</span>
<span class="sd">            - Dataset: Training subset.</span>
<span class="sd">            - Dataset: Evaluation subset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train_data</span><span class="p">,</span> <span class="n">eval_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span>
                                             <span class="n">test_size</span><span class="o">=</span><span class="n">eval_size</span><span class="p">,</span>
                                             <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">eval_data</span></div>



<div class="viewcode-block" id="initialize">
<a class="viewcode-back" href="../../lingowiz.html#lingowiz.train.initialize">[docs]</a>
<span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="n">data_train</span><span class="p">,</span>
               <span class="n">data_eval</span><span class="p">,</span>
               <span class="n">base_model</span><span class="p">,</span>
               <span class="n">src</span><span class="p">,</span>
               <span class="n">trg</span><span class="p">,</span>
               <span class="n">layer</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the tokenizer, model, and tokenized datasets for training.</span>

<span class="sd">    Args:</span>
<span class="sd">        data_train (Dataset): Training dataset.</span>
<span class="sd">        data_eval (Dataset): Evaluation dataset.</span>
<span class="sd">        special_model (str): Special model configuration (optional).</span>
<span class="sd">        base_model (str): Pretrained MarianMT model name or path.</span>
<span class="sd">        src (str): Source language column in the dataset.</span>
<span class="sd">        trg (str): Target language column in the dataset.</span>
<span class="sd">        layer (int, optional): Layer freezing configuration. Defaults to 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple: A tuple containing:</span>
<span class="sd">            - Dataset: Tokenized training dataset.</span>
<span class="sd">            - Dataset: Tokenized evaluation dataset.</span>
<span class="sd">            - MarianMTModel: Initialized MarianMT model.</span>
<span class="sd">            - MarianTokenizer: Initialized tokenizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the tokenizer from the pre-trained MarianMT model</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intializing Tokenizer ...&quot;</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">MarianTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing Model ...&quot;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MarianMTModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">)</span>

    <span class="c1"># print(&quot;Freezing embedding layer&quot;)</span>
    <span class="c1"># model.model.shared.requires_grad = False</span>

    <span class="c1"># # Unfreeze encoder layer for source language encoding</span>
    <span class="c1"># print(f&quot;Freezing last layers of encoder ...&quot;)</span>
    <span class="c1"># for layer in model.model.encoder.layers[:-1]:</span>
    <span class="c1">#   for param in layer.parameters():</span>
    <span class="c1">#     param.requires_grad = False  # Freeze the encoder layers</span>

    <span class="c1"># Unfreeze decoder layer for target language generation</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Freezing last layers of decoder ...&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">tokenizer_args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;tokenizer&quot;</span><span class="p">:</span> <span class="n">tokenizer</span><span class="p">,</span>
                      <span class="s2">&quot;src&quot;</span><span class="p">:</span> <span class="n">src</span><span class="p">,</span>
                      <span class="s2">&quot;trg&quot;</span><span class="p">:</span> <span class="n">trg</span><span class="p">}</span>
    <span class="c1"># Apply tokenization to the train dataset in batches (batched=True)</span>
    <span class="n">tokenized_datasets_train</span> <span class="o">=</span> <span class="n">data_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span>
                                              <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                              <span class="n">fn_kwargs</span><span class="o">=</span><span class="n">tokenizer_args</span><span class="p">)</span>
    <span class="n">tokenized_datasets_eval</span> <span class="o">=</span> <span class="n">data_eval</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span>
                                            <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                            <span class="n">fn_kwargs</span><span class="o">=</span><span class="n">tokenizer_args</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialization Complete&quot;</span><span class="p">)</span>

    <span class="c1"># Return the tokenized dataset, the model,</span>
    <span class="c1"># and the tokenizer for further use</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tokenized_datasets_train</span><span class="p">,</span>
            <span class="n">tokenized_datasets_eval</span><span class="p">,</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">tokenizer</span><span class="p">)</span></div>



<div class="viewcode-block" id="log_params">
<a class="viewcode-back" href="../../lingowiz.html#lingowiz.train.log_params">[docs]</a>
<span class="k">def</span> <span class="nf">log_params</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span>
               <span class="n">batch_size</span><span class="p">,</span>
               <span class="n">learning_rate</span><span class="p">,</span>
               <span class="n">epochs</span><span class="p">,</span>
               <span class="n">warmup_steps</span><span class="p">,</span>
               <span class="n">experiment_name</span><span class="p">,</span>
               <span class="n">df</span><span class="p">,</span>
               <span class="n">log_history</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logs training hyperparameters, data, and model metadata to MLflow.</span>

<span class="sd">    Args:</span>
<span class="sd">        base_model (str): Base MarianMT model used for training.</span>
<span class="sd">        steps (int): Total training steps.</span>
<span class="sd">        batch_size (int): Batch size for training and evaluation.</span>
<span class="sd">        learning_rate (float): Learning rate used during training.</span>
<span class="sd">        epochs (int): Number of training epochs.</span>
<span class="sd">        warmup_steps (int): Number of warmup steps for learning rate scheduling.</span>
<span class="sd">        experiment_name (str): Name of the MLflow experiment.</span>
<span class="sd">        df (pd.DataFrame): Training dataset.</span>
<span class="sd">        model (MarianMTModel): Trained MarianMT model.</span>
<span class="sd">        tokenizer (MarianTokenizer): Tokenizer used for training.</span>
<span class="sd">        log_history (list): Training logs for parameter logging.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">now</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
    <span class="n">formatted_date_time</span> <span class="o">=</span> <span class="n">now</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">_%H:%M&quot;</span><span class="p">)</span>
    <span class="c1"># Create or set the experiment</span>

    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="n">experiment_name</span><span class="p">)</span>
    <span class="n">run_name</span> <span class="o">=</span> <span class="n">experiment_name</span><span class="o">+</span><span class="s2">&quot;_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">formatted_date_time</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">(</span><span class="n">run_name</span><span class="o">=</span><span class="n">run_name</span><span class="p">,</span>
                          <span class="n">nested</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logging Hyperparameters&quot;</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">&quot;epochs&quot;</span><span class="p">,</span> <span class="n">epochs</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">&quot;base_model&quot;</span><span class="p">,</span> <span class="n">base_model</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">&quot;warmup_steps&quot;</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logging Data&quot;</span><span class="p">)</span>
        <span class="n">dataset</span><span class="p">:</span> <span class="n">PandasDataset</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df</span><span class="p">,</span>
                                                         <span class="n">source</span><span class="o">=</span><span class="s2">&quot;dataset_train&quot;</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_input</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logging Model Metrics and Params&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">log_history</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]:</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logging Model...&quot;</span><span class="p">)</span>
        <span class="n">body</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;model_name&quot;</span><span class="p">:</span> <span class="n">experiment_name</span><span class="p">,</span>
            <span class="s2">&quot;run_name&quot;</span><span class="p">:</span> <span class="n">run_name</span>
        <span class="p">}</span>
        <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;https://ideal-amoeba-specially.ngrok-free.app/mlflow&quot;</span><span class="p">,</span>
                      <span class="n">json</span><span class="o">=</span><span class="n">body</span><span class="p">,</span>
                      <span class="n">verify</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logging Complete&quot;</span><span class="p">)</span></div>



<div class="viewcode-block" id="training_pipeline">
<a class="viewcode-back" href="../../lingowiz.html#lingowiz.train.training_pipeline">[docs]</a>
<span class="k">def</span> <span class="nf">training_pipeline</span><span class="p">(</span><span class="n">df</span><span class="p">,</span>
                      <span class="n">src</span><span class="p">,</span>
                      <span class="n">base_model</span><span class="p">,</span>
                      <span class="n">steps</span><span class="p">,</span>
                      <span class="n">batch_size</span><span class="p">,</span>
                      <span class="n">learning_rate</span><span class="p">,</span>
                      <span class="n">epochs</span><span class="p">,</span>
                      <span class="n">warmup</span><span class="p">,</span>
                      <span class="n">special_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">trg_language</span><span class="o">=</span><span class="s2">&quot;Arabic&quot;</span><span class="p">,</span>
                      <span class="n">layer</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Executes the end-to-end training pipeline for fine-tuning a MarianMT model.</span>

<span class="sd">    Args:</span>
<span class="sd">        df (pd.DataFrame): Input dataset for training and evaluation.</span>
<span class="sd">        src (str): Source language column name in the dataset.</span>
<span class="sd">        base_model (str): Pretrained MarianMT model name or path.</span>
<span class="sd">        steps (int): Total training steps.</span>
<span class="sd">        batch_size (int): Batch size for training and evaluation.</span>
<span class="sd">        learning_rate (float): Learning rate for fine-tuning.</span>
<span class="sd">        epochs (int): Number of training epochs.</span>
<span class="sd">        warmup (int): Number of warmup steps for learning rate scheduling.</span>
<span class="sd">        special_model (str, optional): Custom model configuration. Defaults to None.</span>
<span class="sd">        trg_language (str, optional): Target language name. Defaults to &quot;Arabic&quot;.</span>
<span class="sd">        layer (int, optional): Layer freezing configuration. Defaults to 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;translator_</span><span class="si">{</span><span class="n">src</span><span class="si">}</span><span class="s2">_Arabic_spec&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data Split train - eval&quot;</span><span class="p">)</span>
    <span class="n">df_train</span><span class="p">,</span> <span class="n">df_eval</span> <span class="o">=</span> <span class="n">split_eval</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">data_train</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
    <span class="n">data_eval</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df_eval</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Length train data </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">Length eval data </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df_eval</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1">#  Initialing model, tokenizer, data</span>
    <span class="p">(</span><span class="n">tokenized_datasets_train</span><span class="p">,</span>
     <span class="n">tokenized_datasets_eval</span><span class="p">,</span>
     <span class="n">model</span><span class="p">,</span>
     <span class="n">tokenizer</span><span class="p">)</span> <span class="o">=</span> <span class="n">initialize</span><span class="p">(</span><span class="n">data_train</span><span class="p">,</span>
                             <span class="n">data_eval</span><span class="p">,</span>
                             <span class="n">special_model</span><span class="p">,</span>
                             <span class="n">base_model</span><span class="p">,</span>
                             <span class="n">src</span><span class="p">,</span>
                             <span class="n">trg_language</span><span class="p">,</span>
                             <span class="n">layer</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">tokenized_datasets_train</span><span class="p">,</span>
        <span class="n">tokenized_datasets_eval</span><span class="p">,</span>
        <span class="n">steps</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">epochs</span><span class="p">,</span>
        <span class="n">warmup</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">,</span>
        <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Complete&quot;</span><span class="p">)</span>

    <span class="n">log_history</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">log_history</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span>
                                  <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_path</span><span class="p">)</span>

    <span class="c1"># Upload to hugging face</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Uploading model to hugging face ...&quot;</span><span class="p">)</span>
    <span class="n">api</span> <span class="o">=</span> <span class="n">HfApi</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">)</span>
    <span class="n">api</span><span class="o">.</span><span class="n">upload_folder</span><span class="p">(</span><span class="n">folder_path</span><span class="o">=</span><span class="n">output_path</span><span class="p">,</span>
                      <span class="n">repo_id</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;patrick844/</span><span class="si">{</span><span class="n">output_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                      <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">)</span>

    <span class="c1"># Logging</span>
    <span class="n">experiment_name</span> <span class="o">=</span> <span class="n">output_path</span>
    <span class="n">log_params</span><span class="p">(</span>
        <span class="n">base_model</span><span class="p">,</span>
        <span class="n">steps</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">epochs</span><span class="p">,</span> <span class="n">warmup</span><span class="p">,</span>
        <span class="n">experiment_name</span><span class="p">,</span>
        <span class="n">df_train</span><span class="p">,</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">log_history</span><span class="p">)</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Patrick Saadde.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>